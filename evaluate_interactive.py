"""
Interactive evaluation tool for testing individual queries.
"""

import requests

from evaluation.metrics import RAGMetrics

API_URL = "http://localhost:8000/api/v1"


def evaluate_single_query():
    """Interactively evaluate a single query."""

    metrics = RAGMetrics()

    print("\n" + "=" * 60)
    print("   ğŸ”¬ SINGLE QUERY EVALUATION")
    print("=" * 60)

    # Get query
    query = input("\nğŸ“ Enter your query: ").strip()
    if not query:
        print("No query provided")
        return

    # Get expected answer
    expected = input("ğŸ“ Enter expected answer: ").strip()
    if not expected:
        print("No expected answer provided")
        return

    # Query the system
    print("\nğŸ”„ Querying RAG system...")

    try:
        response = requests.post(
            f"{API_URL}/query",
            json={"query": query, "top_k": 5, "include_sources": True},
            timeout=120,
        )
        result = response.json()
    except Exception as e:
        print(f"âŒ Error: {e}")
        return

    generated = result.get("answer", "")
    sources = result.get("sources", [])
    context = "\n".join([s.get("content", "") for s in sources])

    # Calculate metrics
    print("\nğŸ“Š Calculating metrics...")

    # Generation metrics
    rouge_1, rouge_2, rouge_l = metrics.calculate_rouge(generated, expected)
    bleu = metrics.calculate_bleu(generated, expected)
    relevance = metrics.calculate_answer_relevance(query, generated)
    faithfulness = metrics.calculate_faithfulness(generated, context)
    completeness = metrics.calculate_answer_completeness(generated, expected)
    correctness = metrics.calculate_answer_correctness(generated, expected)

    # Display results
    print("\n" + "=" * 60)
    print("ğŸ“Š EVALUATION RESULTS")
    print("=" * 60)

    print(f"\nğŸ”¹ Query: {query}")
    print(f"\nğŸ”¹ Expected Answer:\n   {expected[:200]}...")
    print(f"\nğŸ”¹ Generated Answer:\n   {generated[:200]}...")

    print(f"\n{'â”€' * 60}")
    print("ğŸ“ˆ METRICS:")
    print(f"{'â”€' * 60}")

    metrics_display = [
        ("Answer Correctness", correctness),
        ("Answer Relevance", relevance),
        ("Faithfulness", faithfulness),
        ("Completeness", completeness),
        ("BLEU Score", bleu),
        ("ROUGE-1", rouge_1),
        ("ROUGE-2", rouge_2),
        ("ROUGE-L", rouge_l),
    ]

    for name, value in metrics_display:
        bar = "â–ˆ" * int(value * 20) + "â–‘" * (20 - int(value * 20))
        status = "âœ…" if value >= 0.5 else "âš ï¸" if value >= 0.3 else "âŒ"
        print(f"   {status} {name:20}: [{bar}] {value:.3f}")

    # Overall assessment
    avg_score = (correctness + relevance + faithfulness + completeness) / 4

    print(f"\n{'â”€' * 60}")
    print(f"ğŸ“Š OVERALL SCORE: {avg_score:.1%}")

    if avg_score >= 0.7:
        print("   âœ… EXCELLENT - The answer is accurate and complete")
    elif avg_score >= 0.5:
        print("   âš ï¸  GOOD - The answer is mostly correct but could be improved")
    elif avg_score >= 0.3:
        print("   âš ï¸  FAIR - The answer needs improvement")
    else:
        print("   âŒ POOR - The answer is incorrect or irrelevant")

    print("=" * 60)


def main():
    while True:
        evaluate_single_query()

        again = input("\n\nEvaluate another query? (y/n): ").strip().lower()
        if again != "y":
            print("\nğŸ‘‹ Goodbye!")
            break


if __name__ == "__main__":
    main()
